{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Gesture Analysis",
   "id": "f8063182a7f858c0"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-11T10:30:37.551775Z",
     "start_time": "2025-05-11T10:30:37.547735Z"
    }
   },
   "source": "# %pip install opencv-python mediapipe",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T11:04:41.194321Z",
     "start_time": "2025-05-13T11:04:41.190568Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ],
   "id": "9e9249f166f336ef",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T10:30:42.000534Z",
     "start_time": "2025-05-11T10:30:37.572841Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "def analyze_gestures(video_path):\n",
    "    mp_pose = mp.solutions.pose\n",
    "    pose = mp_pose.Pose(static_image_mode=False, min_detection_confidence=0.5)\n",
    "\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frame_count = 0\n",
    "    valid_frame_count = 0\n",
    "    gesture_movement = []\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame_count += 1\n",
    "        image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        result = pose.process(image_rgb)\n",
    "\n",
    "        if result.pose_landmarks:\n",
    "            lm = result.pose_landmarks.landmark\n",
    "            required = [\n",
    "                mp_pose.PoseLandmark.LEFT_SHOULDER,\n",
    "                mp_pose.PoseLandmark.LEFT_WRIST,\n",
    "                mp_pose.PoseLandmark.RIGHT_SHOULDER,\n",
    "                mp_pose.PoseLandmark.RIGHT_WRIST\n",
    "            ]\n",
    "\n",
    "            if all(lm[i].visibility > 0.5 for i in required):\n",
    "                def dist(a, b):\n",
    "                    return np.sqrt((a.x - b.x)**2 + (a.y - b.y)**2)\n",
    "\n",
    "                l_dist = dist(lm[mp_pose.PoseLandmark.LEFT_SHOULDER], lm[mp_pose.PoseLandmark.LEFT_WRIST])\n",
    "                r_dist = dist(lm[mp_pose.PoseLandmark.RIGHT_SHOULDER], lm[mp_pose.PoseLandmark.RIGHT_WRIST])\n",
    "                gesture_movement.append((frame_count, l_dist, r_dist))\n",
    "                valid_frame_count += 1\n",
    "\n",
    "    cap.release()\n",
    "    pose.close()\n",
    "\n",
    "    if not gesture_movement:\n",
    "        return {\n",
    "            \"Error\": \"Недостаточно кадров с хорошо видимыми жестами\"\n",
    "            \n",
    "        }\n",
    "\n",
    "    gesture_movement = np.array(gesture_movement)\n",
    "    left_var = np.var(gesture_movement[:, 1])\n",
    "    right_var = np.var(gesture_movement[:, 2])\n",
    "    avg_gesture_amp = np.mean((gesture_movement[:, 1] + gesture_movement[:, 2]) / 2)\n",
    "\n",
    "    return {\n",
    "        \"left_hand_variability\": round(left_var, 5),\n",
    "        \"right_hand_variability\": round(right_var, 5),\n",
    "        \"avg_hand_movement_amplitude\": round(avg_gesture_amp, 5),\n",
    "        \"valid_frames_analyzed\": valid_frame_count,\n",
    "        \"total_frames\": frame_count,\n",
    "        \"coverage_%\": round(100 * valid_frame_count / frame_count, 1)\n",
    "    }\n"
   ],
   "id": "91f6be7e7971bbbd",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T10:31:06.176222Z",
     "start_time": "2025-05-11T10:30:42.015805Z"
    }
   },
   "cell_type": "code",
   "source": [
    "video_path = \"data\\\\fragments\\\\videos\\\\8Dv2Hdf5TRg\\\\8Dv2Hdf5TRg_seg000.mp4\"\n",
    "result = analyze_gestures(video_path)"
   ],
   "id": "9f3742e113ad417d",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T10:31:06.204023Z",
     "start_time": "2025-05-11T10:31:06.192835Z"
    }
   },
   "cell_type": "code",
   "source": "result",
   "id": "771a430b8b72fd",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'left_hand_variability': 0.00914,\n",
       " 'right_hand_variability': 0.01406,\n",
       " 'avg_hand_movement_amplitude': 0.37095,\n",
       " 'valid_frames_analyzed': 366,\n",
       " 'total_frames': 733,\n",
       " 'coverage_%': 49.9}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T10:33:39.563046Z",
     "start_time": "2025-05-11T10:33:39.533019Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data = pd.read_excel(\"data/fragments/dataset.xlsx\")\n",
    "\n",
    "features = ['left_hand_variability', 'right_hand_variability', 'avg_hand_movement_amplitude', 'valid_frames_analyzed', 'total_frames', 'coverage_%']\n",
    "data[features] = np.float64(0)"
   ],
   "id": "7ea1247c9127b948",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# import os\n",
    "# \n",
    "# videos = \"data\\\\fragments\\\\videos\"\n",
    "# for video in os.listdir(videos):\n",
    "#     video_path = os.path.join(videos, video)\n",
    "#     for segment in os.listdir(video_path):\n",
    "#         segment_path = os.path.join(video_path, segment)\n",
    "#         result = analyze_gestures(segment_path)\n",
    "#         \n",
    "#         if \"Error\" in result:\n",
    "#             print(f\"Error in {segment_path}: {result['Error']}\")\n",
    "#             continue\n",
    "#         \n",
    "#         data.loc[data['segment_name'] == segment[:-4], features] = list(result.values())\n",
    "# \n",
    "# data.to_csv(\"dataset_gesture.csv\", index=False)"
   ],
   "id": "b954851e09d1b3f5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T14:29:14.484703Z",
     "start_time": "2025-05-11T14:29:14.476909Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# with open(\"C:\\\\Users\\\\User\\\\OneDrive - НИУ Высшая школа экономики\\\\Рабочий стол\\\\output.txt\", 'r') as f:\n",
    "#     lines = f.readlines()\n",
    "# \n",
    "# video_with_errors = [line.split()[2][:-1].split('\\\\')[-1][:-4] for line in lines]\n",
    "# \n",
    "# data = data[~data['segment_name'].isin(video_with_errors)]"
   ],
   "id": "d996906f5da8bd81",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T11:10:43.457762Z",
     "start_time": "2025-05-13T11:10:43.437125Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data = pd.read_csv(\"D:\\\\course_project\\\\data\\\\datasets\\\\dataset_gesture_cleaned.csv\")\n",
    "data.columns"
   ],
   "id": "8dd2924f1f69ee79",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['segment_name', 'confidence_assessment',\n",
       "       'mediapipe_left_hand_variability', 'mediapipe_right_hand_variability',\n",
       "       'mediapipe_avg_hand_movement_amplitude',\n",
       "       'mediapipe_valid_frames_analyzed', 'mediapipe_total_frames',\n",
       "       'mediapipe_coverage_%'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T11:10:32.667051Z",
     "start_time": "2025-05-13T11:10:32.657223Z"
    }
   },
   "cell_type": "code",
   "source": [
    "mediapipe = ['left_hand_variability', 'right_hand_variability', 'avg_hand_movement_amplitude', 'valid_frames_analyzed', 'total_frames', 'coverage_%']\n",
    "mediapipe = ['mediapipe_' + i for i in mediapipe]\n",
    "\n",
    "new_columns = ['segment_name', 'confidence_assessment'] + mediapipe\n",
    "data.columns = new_columns\n",
    "\n",
    "data.to_csv(\"D:\\\\course_project\\\\data\\\\datasets\\\\dataset_gesture_cleaned.csv\", index=False)"
   ],
   "id": "3c9401033ca54fb0",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "---",
   "id": "1695c67efdb2460d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T17:17:05.552632Z",
     "start_time": "2025-05-11T17:16:37.592885Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "# Initialize MediaPipe pose and drawing utilities\n",
    "mp_pose = mp.solutions.pose\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Load your video file\n",
    "video_path = 'data/fragments/videos/Ks-_Mh1QhMc/Ks-_Mh1QhMc_seg035.mp4'  # <- Replace with your video file path\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Optional: get video writer to save output\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter('pose_output.mp4', fourcc, cap.get(cv2.CAP_PROP_FPS),\n",
    "                      (int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)), int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))))\n",
    "\n",
    "with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Convert the frame to RGB\n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False\n",
    "\n",
    "        # Get pose landmarks\n",
    "        results = pose.process(image)\n",
    "\n",
    "        # Convert back to BGR for rendering\n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        # Draw pose landmarks and connections\n",
    "        if results.pose_landmarks:\n",
    "            mp_drawing.draw_landmarks(\n",
    "                image,\n",
    "                results.pose_landmarks,\n",
    "                mp_pose.POSE_CONNECTIONS,\n",
    "                mp_drawing.DrawingSpec(color=(0,255,0), thickness=2, circle_radius=2),\n",
    "                mp_drawing.DrawingSpec(color=(0,0,255), thickness=2)\n",
    "            )\n",
    "\n",
    "        # Write the frame to output file\n",
    "        out.write(image)\n",
    "\n",
    "        # Optionally show the frame\n",
    "        cv2.imshow('MediaPipe Pose on Video', image)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()\n"
   ],
   "id": "1e7263dd46bb4b11",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "647b2fbc674ab02d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
